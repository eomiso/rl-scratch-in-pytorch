{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59335a4",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic\n",
    "actor critic using value function as a baseline.(vanilla actor critic)\n",
    "\n",
    "https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f0d2739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "48d4fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "render = True\n",
    "log_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3e1c2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, beta, input_dims, n_actions, name='actor-critic', fc1_dims=128):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.name = name\n",
    "        \n",
    "        # fc1 is the common graph\n",
    "        self.fc1 = nn.Linear(self.input_dims[0], self.fc1_dims)\n",
    "        self.val = nn.Linear(self.fc1_dims, 1)\n",
    "        self.act = nn.Linear(self.fc1_dims, n_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        common = self.fc1(state)\n",
    "        common = F.relu(common)\n",
    "        state_val = self.val(common)\n",
    "        act_dist = F.softmax(self.act(common), dim=1)\n",
    "        \n",
    "        return act_dist, state_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d35db62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "class Agent:\n",
    "    def __init__(self, beta, input_dims=[8], env=None, gamma=0.99, n_actions=2, fc1_dims=128):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        #self.max_step_size = max_step_size\n",
    "        \n",
    "        self.actorcritic = ActorCriticNetwork(beta, input_dims, n_actions, name='actor-critic')\n",
    "        \n",
    "        self.actor_losses = [] # policy losses\n",
    "        self.critic_losses = [] # value losses\n",
    "        \n",
    "        \n",
    "        self.value_buffer = []\n",
    "        self.action_log_prob = []\n",
    "        self.reward_buffer = []\n",
    "        #self.results = [] # true result values\n",
    "        \n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "    \n",
    "    def init_losses(self):\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = T.Tensor([observation]).to(self.actorcritic.device)\n",
    "        act_dist, state_val = self.actorcritic.forward(state)\n",
    "        \n",
    "        m = Categorical(T.Tensor(act_dist))\n",
    "        action = m.sample()\n",
    "        action_log_prob = m.log_prob(action)\n",
    "        \n",
    "        return action, action_log_prob, state_val\n",
    "    \n",
    "    def init_value_action_log_prob_buffer(self):\n",
    "        self.action_log_prob = []\n",
    "        self.value_buffer = []\n",
    "    \n",
    "    def save_buffer(self, action_log_prob, state_val):\n",
    "        self.value_buffer.append(state_val)\n",
    "        self.action_log_prob.append(action_log_prob)\n",
    "    \n",
    "    def init_reward_buffer(self):\n",
    "        self.reward_buffer = []\n",
    "        \n",
    "    def save_reward(self, reward):\n",
    "        self.reward_buffer.append(reward)\n",
    "    \n",
    "    def get_expected_returns(self):\n",
    "        # the true expected returns\n",
    "        returns = []\n",
    "        R = 0\n",
    "        \n",
    "        for r in self.reward_buffer[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0,R)\n",
    "        \n",
    "        returns = T.Tensor(returns)\n",
    "        # regularization of returns\n",
    "        returns = (returns - returns.mean()) / (returns.std() + self.eps)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def learn(self):\n",
    "        self.init_losses()\n",
    "        \n",
    "        returns = self.get_expected_returns()\n",
    "        for log_prob, value, R in zip(self.action_log_prob, self.value_buffer, returns):\n",
    "            advantage = R - value\n",
    "            \n",
    "            self.actor_losses.append(-log_prob * advantage)\n",
    "            criterion_for_val_func = nn.HuberLoss()\n",
    "            self.critic_losses.append(criterion_for_val_func(value, R))\n",
    "\n",
    "        loss = T.stack(self.actor_losses).sum() + T.stack(self.critic_losses).sum()\n",
    "\n",
    "        T.autograd.set_detect_anomaly(True)\n",
    "\n",
    "        self.actorcritic.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.actorcritic.optimizer.step()\n",
    "        \n",
    "        self.init_reward_buffer()\n",
    "        self.init_value_action_log_prob_buffer()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "12f89e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    def __init__(self, beta, gamma, max_step_size=300, n_episodes=100000, seed=44, env='CartPole-v0', fc1_dims=128):\n",
    "        self.env = gym.make(env)\n",
    "        self.env.seed(seed)\n",
    "        T.manual_seed(seed)\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "        #print(self.env.observation_space.shape)\n",
    "        #print(self.env.action_space.n)\n",
    "        self.agent = Agent(beta, self.env.observation_space.shape, self.env, gamma, self.env.action_space.n, fc1_dims)\n",
    "        self.n_episodes = n_episodes\n",
    "        \n",
    "        self.running_reward = 10\n",
    "        self.max_steps = max_step_size\n",
    "    \n",
    "    \n",
    "        \n",
    "    def run_episode(self, init_state):\n",
    "        self.ep_reward = 0\n",
    "        \n",
    "        self.agent.init_value_action_log_prob_buffer()\n",
    "        self.agent.init_reward_buffer()\n",
    "        \n",
    "        state = init_state\n",
    "        for t in range(self.max_steps):\n",
    "            action, action_log_prob, state_val = self.agent.choose_action(state)\n",
    "            self.agent.save_buffer(action_log_prob, state_val)\n",
    "            \n",
    "            action = action.cpu().detach().numpy()[0]\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            \n",
    "            #print(self.env)\n",
    "            if render:\n",
    "                self.env.render()\n",
    "            \n",
    "            self.agent.save_reward(reward)\n",
    "            self.ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # update cumulative reward\n",
    "        self.running_reward = 0.05 * self.ep_reward + (1-0.05) * self.running_reward\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        self.running_reward = 10\n",
    "        \n",
    "        for i_episode in range(self.n_episodes):\n",
    "            init_state = self.env.reset()\n",
    "            \n",
    "            self.run_episode(init_state)\n",
    "            self.agent.learn()\n",
    "            \n",
    "            # log results\n",
    "            if i_episode % log_interval == 0:\n",
    "                print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                      i_episode, self.ep_reward, self.running_reward))\n",
    "            \n",
    "            if self.running_reward > self.env.spec.reward_threshold:\n",
    "                print(\"Solved! Running reward is now {} and \\\n",
    "                       the last episode runs to {} time steps!\".format(self.running_reward, t))\n",
    "                break\n",
    "            \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4798cd83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: 14.00\tAverage reward: 10.20\n",
      "Episode 1\tLast reward: 11.00\tAverage reward: 10.24\n",
      "Episode 2\tLast reward: 19.00\tAverage reward: 10.68\n",
      "Episode 3\tLast reward: 24.00\tAverage reward: 11.34\n",
      "Episode 4\tLast reward: 27.00\tAverage reward: 12.13\n",
      "Episode 5\tLast reward: 35.00\tAverage reward: 13.27\n",
      "Episode 6\tLast reward: 36.00\tAverage reward: 14.41\n",
      "Episode 7\tLast reward: 16.00\tAverage reward: 14.49\n",
      "Episode 8\tLast reward: 21.00\tAverage reward: 14.81\n",
      "Episode 9\tLast reward: 17.00\tAverage reward: 14.92\n",
      "Episode 10\tLast reward: 10.00\tAverage reward: 14.68\n",
      "Episode 11\tLast reward: 41.00\tAverage reward: 15.99\n",
      "Episode 12\tLast reward: 15.00\tAverage reward: 15.94\n",
      "Episode 13\tLast reward: 10.00\tAverage reward: 15.65\n",
      "Episode 14\tLast reward: 12.00\tAverage reward: 15.46\n",
      "Episode 15\tLast reward: 9.00\tAverage reward: 15.14\n",
      "Episode 16\tLast reward: 23.00\tAverage reward: 15.53\n",
      "Episode 17\tLast reward: 14.00\tAverage reward: 15.46\n",
      "Episode 18\tLast reward: 17.00\tAverage reward: 15.53\n",
      "Episode 19\tLast reward: 33.00\tAverage reward: 16.41\n",
      "Episode 20\tLast reward: 13.00\tAverage reward: 16.24\n",
      "Episode 21\tLast reward: 13.00\tAverage reward: 16.07\n",
      "Episode 22\tLast reward: 14.00\tAverage reward: 15.97\n",
      "Episode 23\tLast reward: 20.00\tAverage reward: 16.17\n",
      "Episode 24\tLast reward: 36.00\tAverage reward: 17.16\n",
      "Episode 25\tLast reward: 31.00\tAverage reward: 17.86\n",
      "Episode 26\tLast reward: 31.00\tAverage reward: 18.51\n",
      "Episode 27\tLast reward: 27.00\tAverage reward: 18.94\n",
      "Episode 28\tLast reward: 42.00\tAverage reward: 20.09\n",
      "Episode 29\tLast reward: 22.00\tAverage reward: 20.19\n",
      "Episode 30\tLast reward: 28.00\tAverage reward: 20.58\n",
      "Episode 31\tLast reward: 27.00\tAverage reward: 20.90\n",
      "Episode 32\tLast reward: 17.00\tAverage reward: 20.70\n",
      "Episode 33\tLast reward: 38.00\tAverage reward: 21.57\n",
      "Episode 34\tLast reward: 12.00\tAverage reward: 21.09\n",
      "Episode 35\tLast reward: 40.00\tAverage reward: 22.03\n",
      "Episode 36\tLast reward: 24.00\tAverage reward: 22.13\n",
      "Episode 37\tLast reward: 52.00\tAverage reward: 23.63\n",
      "Episode 38\tLast reward: 62.00\tAverage reward: 25.55\n",
      "Episode 39\tLast reward: 31.00\tAverage reward: 25.82\n",
      "Episode 40\tLast reward: 74.00\tAverage reward: 28.23\n",
      "Episode 41\tLast reward: 61.00\tAverage reward: 29.87\n",
      "Episode 42\tLast reward: 28.00\tAverage reward: 29.77\n",
      "Episode 43\tLast reward: 22.00\tAverage reward: 29.38\n",
      "Episode 44\tLast reward: 19.00\tAverage reward: 28.86\n",
      "Episode 45\tLast reward: 112.00\tAverage reward: 33.02\n",
      "Episode 46\tLast reward: 34.00\tAverage reward: 33.07\n",
      "Episode 47\tLast reward: 33.00\tAverage reward: 33.07\n",
      "Episode 48\tLast reward: 21.00\tAverage reward: 32.46\n",
      "Episode 49\tLast reward: 28.00\tAverage reward: 32.24\n",
      "Episode 50\tLast reward: 25.00\tAverage reward: 31.88\n",
      "Episode 51\tLast reward: 35.00\tAverage reward: 32.03\n",
      "Episode 52\tLast reward: 22.00\tAverage reward: 31.53\n",
      "Episode 53\tLast reward: 21.00\tAverage reward: 31.01\n",
      "Episode 54\tLast reward: 22.00\tAverage reward: 30.56\n",
      "Episode 55\tLast reward: 18.00\tAverage reward: 29.93\n",
      "Episode 56\tLast reward: 34.00\tAverage reward: 30.13\n",
      "Episode 57\tLast reward: 22.00\tAverage reward: 29.72\n",
      "Episode 58\tLast reward: 25.00\tAverage reward: 29.49\n",
      "Episode 59\tLast reward: 15.00\tAverage reward: 28.76\n",
      "Episode 60\tLast reward: 77.00\tAverage reward: 31.18\n",
      "Episode 61\tLast reward: 18.00\tAverage reward: 30.52\n",
      "Episode 62\tLast reward: 36.00\tAverage reward: 30.79\n",
      "Episode 63\tLast reward: 13.00\tAverage reward: 29.90\n",
      "Episode 64\tLast reward: 24.00\tAverage reward: 29.61\n",
      "Episode 65\tLast reward: 20.00\tAverage reward: 29.13\n",
      "Episode 66\tLast reward: 39.00\tAverage reward: 29.62\n",
      "Episode 67\tLast reward: 46.00\tAverage reward: 30.44\n",
      "Episode 68\tLast reward: 90.00\tAverage reward: 33.42\n",
      "Episode 69\tLast reward: 52.00\tAverage reward: 34.35\n",
      "Episode 70\tLast reward: 68.00\tAverage reward: 36.03\n",
      "Episode 71\tLast reward: 17.00\tAverage reward: 35.08\n",
      "Episode 72\tLast reward: 74.00\tAverage reward: 37.02\n",
      "Episode 73\tLast reward: 33.00\tAverage reward: 36.82\n",
      "Episode 74\tLast reward: 60.00\tAverage reward: 37.98\n",
      "Episode 75\tLast reward: 57.00\tAverage reward: 38.93\n",
      "Episode 76\tLast reward: 117.00\tAverage reward: 42.84\n",
      "Episode 77\tLast reward: 153.00\tAverage reward: 48.34\n",
      "Episode 78\tLast reward: 140.00\tAverage reward: 52.93\n",
      "Episode 79\tLast reward: 160.00\tAverage reward: 58.28\n",
      "Episode 80\tLast reward: 154.00\tAverage reward: 63.07\n",
      "Episode 81\tLast reward: 36.00\tAverage reward: 61.71\n",
      "Episode 82\tLast reward: 61.00\tAverage reward: 61.68\n",
      "Episode 83\tLast reward: 33.00\tAverage reward: 60.24\n",
      "Episode 84\tLast reward: 200.00\tAverage reward: 67.23\n",
      "Episode 85\tLast reward: 133.00\tAverage reward: 70.52\n",
      "Episode 86\tLast reward: 170.00\tAverage reward: 75.49\n",
      "Episode 87\tLast reward: 200.00\tAverage reward: 81.72\n",
      "Episode 88\tLast reward: 176.00\tAverage reward: 86.43\n",
      "Episode 89\tLast reward: 199.00\tAverage reward: 92.06\n",
      "Episode 90\tLast reward: 148.00\tAverage reward: 94.86\n",
      "Episode 91\tLast reward: 200.00\tAverage reward: 100.12\n",
      "Episode 92\tLast reward: 105.00\tAverage reward: 100.36\n",
      "Episode 93\tLast reward: 200.00\tAverage reward: 105.34\n",
      "Episode 94\tLast reward: 29.00\tAverage reward: 101.52\n",
      "Episode 95\tLast reward: 94.00\tAverage reward: 101.15\n",
      "Episode 96\tLast reward: 96.00\tAverage reward: 100.89\n",
      "Episode 97\tLast reward: 106.00\tAverage reward: 101.15\n",
      "Episode 98\tLast reward: 95.00\tAverage reward: 100.84\n",
      "Episode 99\tLast reward: 33.00\tAverage reward: 97.45\n",
      "Episode 100\tLast reward: 36.00\tAverage reward: 94.37\n",
      "Episode 101\tLast reward: 30.00\tAverage reward: 91.16\n",
      "Episode 102\tLast reward: 24.00\tAverage reward: 87.80\n",
      "Episode 103\tLast reward: 102.00\tAverage reward: 88.51\n",
      "Episode 104\tLast reward: 37.00\tAverage reward: 85.93\n",
      "Episode 105\tLast reward: 98.00\tAverage reward: 86.54\n",
      "Episode 106\tLast reward: 28.00\tAverage reward: 83.61\n",
      "Episode 107\tLast reward: 39.00\tAverage reward: 81.38\n",
      "Episode 108\tLast reward: 26.00\tAverage reward: 78.61\n",
      "Episode 109\tLast reward: 136.00\tAverage reward: 81.48\n",
      "Episode 110\tLast reward: 96.00\tAverage reward: 82.21\n",
      "Episode 111\tLast reward: 103.00\tAverage reward: 83.25\n",
      "Episode 112\tLast reward: 60.00\tAverage reward: 82.08\n",
      "Episode 113\tLast reward: 31.00\tAverage reward: 79.53\n",
      "Episode 114\tLast reward: 37.00\tAverage reward: 77.40\n",
      "Episode 115\tLast reward: 87.00\tAverage reward: 77.88\n",
      "Episode 116\tLast reward: 150.00\tAverage reward: 81.49\n",
      "Episode 117\tLast reward: 200.00\tAverage reward: 87.41\n",
      "Episode 118\tLast reward: 200.00\tAverage reward: 93.04\n",
      "Episode 119\tLast reward: 178.00\tAverage reward: 97.29\n",
      "Episode 120\tLast reward: 139.00\tAverage reward: 99.38\n",
      "Episode 121\tLast reward: 200.00\tAverage reward: 104.41\n",
      "Episode 122\tLast reward: 160.00\tAverage reward: 107.19\n",
      "Episode 123\tLast reward: 117.00\tAverage reward: 107.68\n",
      "Episode 124\tLast reward: 137.00\tAverage reward: 109.14\n",
      "Episode 125\tLast reward: 174.00\tAverage reward: 112.39\n",
      "Episode 126\tLast reward: 127.00\tAverage reward: 113.12\n",
      "Episode 127\tLast reward: 174.00\tAverage reward: 116.16\n",
      "Episode 128\tLast reward: 141.00\tAverage reward: 117.40\n",
      "Episode 129\tLast reward: 157.00\tAverage reward: 119.38\n",
      "Episode 130\tLast reward: 123.00\tAverage reward: 119.56\n",
      "Episode 131\tLast reward: 148.00\tAverage reward: 120.99\n",
      "Episode 132\tLast reward: 113.00\tAverage reward: 120.59\n",
      "Episode 133\tLast reward: 104.00\tAverage reward: 119.76\n",
      "Episode 134\tLast reward: 142.00\tAverage reward: 120.87\n",
      "Episode 135\tLast reward: 152.00\tAverage reward: 122.43\n",
      "Episode 136\tLast reward: 200.00\tAverage reward: 126.30\n",
      "Episode 137\tLast reward: 127.00\tAverage reward: 126.34\n",
      "Episode 138\tLast reward: 129.00\tAverage reward: 126.47\n",
      "Episode 139\tLast reward: 157.00\tAverage reward: 128.00\n",
      "Episode 140\tLast reward: 129.00\tAverage reward: 128.05\n",
      "Episode 141\tLast reward: 142.00\tAverage reward: 128.75\n",
      "Episode 142\tLast reward: 17.00\tAverage reward: 123.16\n",
      "Episode 143\tLast reward: 147.00\tAverage reward: 124.35\n",
      "Episode 144\tLast reward: 108.00\tAverage reward: 123.53\n",
      "Episode 145\tLast reward: 142.00\tAverage reward: 124.46\n",
      "Episode 146\tLast reward: 150.00\tAverage reward: 125.73\n",
      "Episode 147\tLast reward: 191.00\tAverage reward: 129.00\n",
      "Episode 148\tLast reward: 132.00\tAverage reward: 129.15\n",
      "Episode 149\tLast reward: 170.00\tAverage reward: 131.19\n",
      "Episode 150\tLast reward: 156.00\tAverage reward: 132.43\n",
      "Episode 151\tLast reward: 200.00\tAverage reward: 135.81\n",
      "Episode 152\tLast reward: 189.00\tAverage reward: 138.47\n",
      "Episode 153\tLast reward: 190.00\tAverage reward: 141.05\n",
      "Episode 154\tLast reward: 200.00\tAverage reward: 143.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 155\tLast reward: 200.00\tAverage reward: 146.79\n",
      "Episode 156\tLast reward: 191.00\tAverage reward: 149.00\n",
      "Episode 157\tLast reward: 119.00\tAverage reward: 147.50\n",
      "Episode 158\tLast reward: 200.00\tAverage reward: 150.13\n",
      "Episode 159\tLast reward: 143.00\tAverage reward: 149.77\n",
      "Episode 160\tLast reward: 26.00\tAverage reward: 143.58\n",
      "Episode 161\tLast reward: 155.00\tAverage reward: 144.15\n",
      "Episode 162\tLast reward: 120.00\tAverage reward: 142.95\n",
      "Episode 163\tLast reward: 107.00\tAverage reward: 141.15\n",
      "Episode 164\tLast reward: 200.00\tAverage reward: 144.09\n",
      "Episode 165\tLast reward: 134.00\tAverage reward: 143.59\n",
      "Episode 166\tLast reward: 153.00\tAverage reward: 144.06\n",
      "Episode 167\tLast reward: 200.00\tAverage reward: 146.85\n",
      "Episode 168\tLast reward: 200.00\tAverage reward: 149.51\n",
      "Episode 169\tLast reward: 188.00\tAverage reward: 151.44\n",
      "Episode 170\tLast reward: 200.00\tAverage reward: 153.86\n",
      "Episode 171\tLast reward: 164.00\tAverage reward: 154.37\n",
      "Episode 172\tLast reward: 166.00\tAverage reward: 154.95\n",
      "Episode 173\tLast reward: 48.00\tAverage reward: 149.61\n",
      "Episode 174\tLast reward: 139.00\tAverage reward: 149.07\n",
      "Episode 175\tLast reward: 140.00\tAverage reward: 148.62\n",
      "Episode 176\tLast reward: 172.00\tAverage reward: 149.79\n",
      "Episode 177\tLast reward: 146.00\tAverage reward: 149.60\n",
      "Episode 178\tLast reward: 98.00\tAverage reward: 147.02\n",
      "Episode 179\tLast reward: 125.00\tAverage reward: 145.92\n",
      "Episode 180\tLast reward: 200.00\tAverage reward: 148.62\n",
      "Episode 181\tLast reward: 175.00\tAverage reward: 149.94\n",
      "Episode 182\tLast reward: 164.00\tAverage reward: 150.65\n",
      "Episode 183\tLast reward: 146.00\tAverage reward: 150.41\n",
      "Episode 184\tLast reward: 200.00\tAverage reward: 152.89\n",
      "Episode 185\tLast reward: 175.00\tAverage reward: 154.00\n",
      "Episode 186\tLast reward: 169.00\tAverage reward: 154.75\n",
      "Episode 187\tLast reward: 129.00\tAverage reward: 153.46\n",
      "Episode 188\tLast reward: 200.00\tAverage reward: 155.79\n",
      "Episode 189\tLast reward: 181.00\tAverage reward: 157.05\n",
      "Episode 190\tLast reward: 200.00\tAverage reward: 159.20\n",
      "Episode 191\tLast reward: 200.00\tAverage reward: 161.24\n",
      "Episode 192\tLast reward: 200.00\tAverage reward: 163.17\n",
      "Episode 193\tLast reward: 200.00\tAverage reward: 165.02\n",
      "Episode 194\tLast reward: 200.00\tAverage reward: 166.76\n",
      "Episode 195\tLast reward: 200.00\tAverage reward: 168.43\n",
      "Episode 196\tLast reward: 200.00\tAverage reward: 170.01\n",
      "Episode 197\tLast reward: 200.00\tAverage reward: 171.50\n",
      "Episode 198\tLast reward: 200.00\tAverage reward: 172.93\n",
      "Episode 199\tLast reward: 200.00\tAverage reward: 174.28\n",
      "Episode 200\tLast reward: 200.00\tAverage reward: 175.57\n",
      "Episode 201\tLast reward: 200.00\tAverage reward: 176.79\n",
      "Episode 202\tLast reward: 200.00\tAverage reward: 177.95\n",
      "Episode 203\tLast reward: 200.00\tAverage reward: 179.05\n",
      "Episode 204\tLast reward: 200.00\tAverage reward: 180.10\n",
      "Episode 205\tLast reward: 200.00\tAverage reward: 181.10\n",
      "Episode 206\tLast reward: 184.00\tAverage reward: 181.24\n",
      "Episode 207\tLast reward: 200.00\tAverage reward: 182.18\n",
      "Episode 208\tLast reward: 191.00\tAverage reward: 182.62\n",
      "Episode 209\tLast reward: 163.00\tAverage reward: 181.64\n",
      "Episode 210\tLast reward: 111.00\tAverage reward: 178.11\n",
      "Episode 211\tLast reward: 157.00\tAverage reward: 177.05\n",
      "Episode 212\tLast reward: 154.00\tAverage reward: 175.90\n",
      "Episode 213\tLast reward: 165.00\tAverage reward: 175.35\n",
      "Episode 214\tLast reward: 165.00\tAverage reward: 174.84\n",
      "Episode 215\tLast reward: 31.00\tAverage reward: 167.64\n",
      "Episode 216\tLast reward: 189.00\tAverage reward: 168.71\n",
      "Episode 217\tLast reward: 186.00\tAverage reward: 169.58\n",
      "Episode 218\tLast reward: 22.00\tAverage reward: 162.20\n",
      "Episode 219\tLast reward: 151.00\tAverage reward: 161.64\n",
      "Episode 220\tLast reward: 200.00\tAverage reward: 163.56\n",
      "Episode 221\tLast reward: 150.00\tAverage reward: 162.88\n",
      "Episode 222\tLast reward: 200.00\tAverage reward: 164.73\n",
      "Episode 223\tLast reward: 200.00\tAverage reward: 166.50\n",
      "Episode 224\tLast reward: 196.00\tAverage reward: 167.97\n",
      "Episode 225\tLast reward: 199.00\tAverage reward: 169.52\n",
      "Episode 226\tLast reward: 113.00\tAverage reward: 166.70\n",
      "Episode 227\tLast reward: 200.00\tAverage reward: 168.36\n",
      "Episode 228\tLast reward: 200.00\tAverage reward: 169.94\n",
      "Episode 229\tLast reward: 200.00\tAverage reward: 171.45\n",
      "Episode 230\tLast reward: 200.00\tAverage reward: 172.88\n",
      "Episode 231\tLast reward: 111.00\tAverage reward: 169.78\n",
      "Episode 232\tLast reward: 151.00\tAverage reward: 168.84\n",
      "Episode 233\tLast reward: 200.00\tAverage reward: 170.40\n",
      "Episode 234\tLast reward: 103.00\tAverage reward: 167.03\n",
      "Episode 235\tLast reward: 146.00\tAverage reward: 165.98\n",
      "Episode 236\tLast reward: 128.00\tAverage reward: 164.08\n",
      "Episode 237\tLast reward: 200.00\tAverage reward: 165.88\n",
      "Episode 238\tLast reward: 200.00\tAverage reward: 167.58\n",
      "Episode 239\tLast reward: 200.00\tAverage reward: 169.20\n",
      "Episode 240\tLast reward: 200.00\tAverage reward: 170.74\n",
      "Episode 241\tLast reward: 200.00\tAverage reward: 172.21\n",
      "Episode 242\tLast reward: 200.00\tAverage reward: 173.60\n",
      "Episode 243\tLast reward: 200.00\tAverage reward: 174.92\n",
      "Episode 244\tLast reward: 173.00\tAverage reward: 174.82\n",
      "Episode 245\tLast reward: 78.00\tAverage reward: 169.98\n",
      "Episode 246\tLast reward: 181.00\tAverage reward: 170.53\n",
      "Episode 247\tLast reward: 200.00\tAverage reward: 172.00\n",
      "Episode 248\tLast reward: 148.00\tAverage reward: 170.80\n",
      "Episode 249\tLast reward: 167.00\tAverage reward: 170.61\n",
      "Episode 250\tLast reward: 164.00\tAverage reward: 170.28\n",
      "Episode 251\tLast reward: 186.00\tAverage reward: 171.07\n",
      "Episode 252\tLast reward: 161.00\tAverage reward: 170.56\n",
      "Episode 253\tLast reward: 119.00\tAverage reward: 167.99\n",
      "Episode 254\tLast reward: 149.00\tAverage reward: 167.04\n",
      "Episode 255\tLast reward: 136.00\tAverage reward: 165.49\n",
      "Episode 256\tLast reward: 133.00\tAverage reward: 163.86\n",
      "Episode 257\tLast reward: 31.00\tAverage reward: 157.22\n",
      "Episode 258\tLast reward: 24.00\tAverage reward: 150.56\n",
      "Episode 259\tLast reward: 129.00\tAverage reward: 149.48\n",
      "Episode 260\tLast reward: 19.00\tAverage reward: 142.96\n",
      "Episode 261\tLast reward: 116.00\tAverage reward: 141.61\n",
      "Episode 262\tLast reward: 115.00\tAverage reward: 140.28\n",
      "Episode 263\tLast reward: 29.00\tAverage reward: 134.71\n",
      "Episode 264\tLast reward: 136.00\tAverage reward: 134.78\n",
      "Episode 265\tLast reward: 110.00\tAverage reward: 133.54\n",
      "Episode 266\tLast reward: 18.00\tAverage reward: 127.76\n",
      "Episode 267\tLast reward: 18.00\tAverage reward: 122.27\n",
      "Episode 268\tLast reward: 28.00\tAverage reward: 117.56\n",
      "Episode 269\tLast reward: 113.00\tAverage reward: 117.33\n",
      "Episode 270\tLast reward: 136.00\tAverage reward: 118.27\n",
      "Episode 271\tLast reward: 142.00\tAverage reward: 119.45\n",
      "Episode 272\tLast reward: 148.00\tAverage reward: 120.88\n",
      "Episode 273\tLast reward: 143.00\tAverage reward: 121.99\n",
      "Episode 274\tLast reward: 132.00\tAverage reward: 122.49\n",
      "Episode 275\tLast reward: 155.00\tAverage reward: 124.11\n",
      "Episode 276\tLast reward: 145.00\tAverage reward: 125.16\n",
      "Episode 277\tLast reward: 152.00\tAverage reward: 126.50\n",
      "Episode 278\tLast reward: 30.00\tAverage reward: 121.67\n",
      "Episode 279\tLast reward: 149.00\tAverage reward: 123.04\n",
      "Episode 280\tLast reward: 143.00\tAverage reward: 124.04\n",
      "Episode 281\tLast reward: 125.00\tAverage reward: 124.09\n",
      "Episode 282\tLast reward: 153.00\tAverage reward: 125.53\n",
      "Episode 283\tLast reward: 145.00\tAverage reward: 126.51\n",
      "Episode 284\tLast reward: 150.00\tAverage reward: 127.68\n",
      "Episode 285\tLast reward: 143.00\tAverage reward: 128.45\n",
      "Episode 286\tLast reward: 148.00\tAverage reward: 129.42\n",
      "Episode 287\tLast reward: 133.00\tAverage reward: 129.60\n",
      "Episode 288\tLast reward: 170.00\tAverage reward: 131.62\n",
      "Episode 289\tLast reward: 145.00\tAverage reward: 132.29\n",
      "Episode 290\tLast reward: 185.00\tAverage reward: 134.93\n",
      "Episode 291\tLast reward: 200.00\tAverage reward: 138.18\n",
      "Episode 292\tLast reward: 185.00\tAverage reward: 140.52\n",
      "Episode 293\tLast reward: 151.00\tAverage reward: 141.05\n",
      "Episode 294\tLast reward: 171.00\tAverage reward: 142.54\n",
      "Episode 295\tLast reward: 183.00\tAverage reward: 144.57\n",
      "Episode 296\tLast reward: 200.00\tAverage reward: 147.34\n",
      "Episode 297\tLast reward: 200.00\tAverage reward: 149.97\n",
      "Episode 298\tLast reward: 199.00\tAverage reward: 152.42\n",
      "Episode 299\tLast reward: 190.00\tAverage reward: 154.30\n",
      "Episode 300\tLast reward: 200.00\tAverage reward: 156.59\n",
      "Episode 301\tLast reward: 198.00\tAverage reward: 158.66\n",
      "Episode 302\tLast reward: 200.00\tAverage reward: 160.72\n",
      "Episode 303\tLast reward: 200.00\tAverage reward: 162.69\n",
      "Episode 304\tLast reward: 190.00\tAverage reward: 164.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 305\tLast reward: 200.00\tAverage reward: 165.85\n",
      "Episode 306\tLast reward: 200.00\tAverage reward: 167.56\n",
      "Episode 307\tLast reward: 186.00\tAverage reward: 168.48\n",
      "Episode 308\tLast reward: 200.00\tAverage reward: 170.06\n",
      "Episode 309\tLast reward: 167.00\tAverage reward: 169.90\n",
      "Episode 310\tLast reward: 200.00\tAverage reward: 171.41\n",
      "Episode 311\tLast reward: 178.00\tAverage reward: 171.74\n",
      "Episode 312\tLast reward: 187.00\tAverage reward: 172.50\n",
      "Episode 313\tLast reward: 166.00\tAverage reward: 172.18\n",
      "Episode 314\tLast reward: 154.00\tAverage reward: 171.27\n",
      "Episode 315\tLast reward: 130.00\tAverage reward: 169.20\n",
      "Episode 316\tLast reward: 136.00\tAverage reward: 167.54\n",
      "Episode 317\tLast reward: 151.00\tAverage reward: 166.72\n",
      "Episode 318\tLast reward: 170.00\tAverage reward: 166.88\n",
      "Episode 319\tLast reward: 157.00\tAverage reward: 166.39\n",
      "Episode 320\tLast reward: 153.00\tAverage reward: 165.72\n",
      "Episode 321\tLast reward: 151.00\tAverage reward: 164.98\n",
      "Episode 322\tLast reward: 181.00\tAverage reward: 165.78\n",
      "Episode 323\tLast reward: 161.00\tAverage reward: 165.54\n",
      "Episode 324\tLast reward: 192.00\tAverage reward: 166.87\n",
      "Episode 325\tLast reward: 200.00\tAverage reward: 168.52\n",
      "Episode 326\tLast reward: 200.00\tAverage reward: 170.10\n",
      "Episode 327\tLast reward: 200.00\tAverage reward: 171.59\n",
      "Episode 328\tLast reward: 196.00\tAverage reward: 172.81\n",
      "Episode 329\tLast reward: 196.00\tAverage reward: 173.97\n",
      "Episode 330\tLast reward: 200.00\tAverage reward: 175.27\n",
      "Episode 331\tLast reward: 200.00\tAverage reward: 176.51\n",
      "Episode 332\tLast reward: 200.00\tAverage reward: 177.68\n",
      "Episode 333\tLast reward: 200.00\tAverage reward: 178.80\n",
      "Episode 334\tLast reward: 200.00\tAverage reward: 179.86\n",
      "Episode 335\tLast reward: 200.00\tAverage reward: 180.87\n",
      "Episode 336\tLast reward: 200.00\tAverage reward: 181.82\n",
      "Episode 337\tLast reward: 200.00\tAverage reward: 182.73\n",
      "Episode 338\tLast reward: 200.00\tAverage reward: 183.60\n",
      "Episode 339\tLast reward: 200.00\tAverage reward: 184.42\n",
      "Episode 340\tLast reward: 200.00\tAverage reward: 185.20\n",
      "Episode 341\tLast reward: 200.00\tAverage reward: 185.94\n",
      "Episode 342\tLast reward: 200.00\tAverage reward: 186.64\n",
      "Episode 343\tLast reward: 200.00\tAverage reward: 187.31\n",
      "Episode 344\tLast reward: 200.00\tAverage reward: 187.94\n",
      "Episode 345\tLast reward: 200.00\tAverage reward: 188.54\n",
      "Episode 346\tLast reward: 200.00\tAverage reward: 189.12\n",
      "Episode 347\tLast reward: 200.00\tAverage reward: 189.66\n",
      "Episode 348\tLast reward: 200.00\tAverage reward: 190.18\n",
      "Episode 349\tLast reward: 200.00\tAverage reward: 190.67\n",
      "Episode 350\tLast reward: 200.00\tAverage reward: 191.14\n",
      "Episode 351\tLast reward: 200.00\tAverage reward: 191.58\n",
      "Episode 352\tLast reward: 200.00\tAverage reward: 192.00\n",
      "Episode 353\tLast reward: 200.00\tAverage reward: 192.40\n",
      "Episode 354\tLast reward: 200.00\tAverage reward: 192.78\n",
      "Episode 355\tLast reward: 200.00\tAverage reward: 193.14\n",
      "Episode 356\tLast reward: 200.00\tAverage reward: 193.48\n",
      "Episode 357\tLast reward: 200.00\tAverage reward: 193.81\n",
      "Episode 358\tLast reward: 200.00\tAverage reward: 194.12\n",
      "Episode 359\tLast reward: 200.00\tAverage reward: 194.41\n",
      "Episode 360\tLast reward: 200.00\tAverage reward: 194.69\n",
      "Episode 361\tLast reward: 200.00\tAverage reward: 194.96\n",
      "Episode 362\tLast reward: 200.00\tAverage reward: 195.21\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'running_reward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-9fd1a872394c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mworker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-129-498187146a02>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_reward\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 print(\"Solved! Running reward is now {} and \\\n\u001b[0;32m---> 59\u001b[0;31m                        the last episode runs to {} time steps!\".format(running_reward, t))\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'running_reward' is not defined"
     ]
    }
   ],
   "source": [
    "worker = Worker(0.01, 0.99)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "start_time = time.time()\n",
    "worker.train()\n",
    "consumed = time.time() - start_time\n",
    "\n",
    "print(str(datetime.timedelta(seconds=consumed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7c0611c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'beta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-5da6dab218cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobersvation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_episodes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'beta' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b29311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
